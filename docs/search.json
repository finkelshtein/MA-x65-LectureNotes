[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA-365/M65 Lecture Notes",
    "section": "",
    "text": "Overview\nUse the left panel to navigate the website.\nUse search field on the left to find all instances of a term in the Lecture Notes.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "Ch-01.html",
    "href": "Ch-01.html",
    "title": "1  Loss distributions",
    "section": "",
    "text": "Examples of continuous distributions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Loss distributions</span>"
    ]
  },
  {
    "objectID": "Ch-01.html#examples-of-continuous-distributions",
    "href": "Ch-01.html#examples-of-continuous-distributions",
    "title": "1  Loss distributions",
    "section": "",
    "text": "1.6 The uniform distribution\n\n\n\nA random variable \\(X:\\Omega\\to\\mathbb{R}\\) has the uniform distribution on \\([a,b]\\subset\\mathbb{R}\\) if its PDF is \\[\n  f(x)=\\frac{1}{b-a}, \\quad x\\in[a,b],\n\\tag{1.36}\\] and \\(f(x)=0\\) otherwise.\nThen the CDF is: \\[\n  F(x)=\\frac{x-a}{b-a}, \\quad x\\in[a,b],\n\\tag{1.37}\\] and \\(F(x)=0\\) for \\(x&lt;a\\), \\(F(x)=1\\) for \\(x&gt;b\\).\nNotation: \\(X\\sim U(a,b)\\).\nFormulas: \\[\n  E(X)=\\frac{a+b}{2},\n\\tag{1.38}\\] \\[\n  \\mathrm{var}(X)=\\frac{1}{12}(b-a)^2,\n\\tag{1.39}\\] \\[\n  \\mathrm{skew}(X)=0.\n\\tag{1.40}\\]\n\n\n\n\n\n\n\n\n1.7 The exponential distribution\n\n\n\nA random variable \\(X:\\Omega\\to{\\mathbb{R}}\\) has the exponential distribution with parameter \\(\\lambda&gt;0\\) if its CDF is \\[\nF(x)=1-e^{-\\lambda x}, \\quad x\\geq0,\n\\tag{1.41}\\] and \\(F(x)=0\\) for \\(x&lt;0\\).\nNotation: \\(X\\sim Exp(\\lambda)\\).\nThe following formulas hold: \\[\nf(x)=\\lambda e^{-\\lambda x}, \\quad x\\geq0.\n\\tag{1.42}\\] \\[\nS(x)=e^{-\\lambda x}, \\quad x\\geq0.\n\\tag{1.43}\\] \\[\n\\mathbb{E}(X)=\\frac1{\\lambda},\n\\tag{1.44}\\] \\[\n\\mathrm{Var}(X)=\\frac1{\\lambda^2}= \\bigl( \\mathbb{E}(X) \\bigr)^2,\n\\tag{1.45}\\] \\[\n\\mathbb{E}(X^2)=\\frac2{\\lambda^2}=2\\bigl( \\mathbb{E}(X) \\bigr)^2.\n\\tag{1.46}\\] More generally: \\[\n  \\mathbb{E}(X^n)=\\frac{n!}{\\lambda^n}=n!\\, \\bigl( \\mathbb{E}(X) \\bigr)^n.\n\\tag{1.47}\\] In particular, \\[\n  \\mathrm{skew}(X)=2.\n\\tag{1.48}\\]\nThe MGM if defined for \\(t&lt;\\lambda\\) only: \\[\nM(t)=\\frac{\\lambda}{\\lambda-t}.\n\\tag{1.49}\\]\nAn exponential random variable obeys the : \\[\n  \\mathbb{P}(X&gt;s+t\\mid X&gt;s) = \\mathbb{P}(X&gt;t), \\quad s,t&gt;0.\n\\tag{1.50}\\]\n\n\n\n\n\n\nviewof l = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\lambda:` })\n\nviewof xmexp = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nymaxexp = d3.max(d3.range(0, xmexp, 0.01).map(t =&gt; l*Math.exp(-l * t)))\n\nPlot.plot({\n  x: { domain: [-0.1, xmexp] },\n  y: { domain: [-0.1*ymaxexp, ymaxexp*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0, xmexp, 0.1)\n        .map(t =&gt; [\n          t,\n          l*Math.exp(-l * t)\n        ]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.1: Graph of \\(f(x)=\\lambda e^{-\\lambda x}\\)\n\n\n\n\n\n\n\n\n\n1.8 The gamma distribution\n\n\n\nA random variable \\(X:\\Omega\\to{\\mathbb{R}}\\) has the gamma distribution with parameters \\(\\alpha&gt;0\\) and \\(\\lambda&gt;0\\) if its PDF is \\[\nf(x)=\\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1} e^{-\\lambda x}, \\quad x\\geq0.\n\\tag{1.51}\\] Here \\(\\Gamma(\\alpha)\\) is the gamma function: \\[\n\\Gamma(\\alpha):=\\int_0^\\infty t^{\\alpha-1}e^{-t}\\,dt, \\quad \\alpha&gt;0\n\\tag{1.52}\\]\nNotation: \\(X\\sim \\Gamma(\\alpha,\\lambda)\\).\nThe following formulas hold: \\[\n\\mathbb{E}(X)=\\frac{\\alpha}{\\lambda},\n\\tag{1.53}\\] \\[\n\\mathrm{Var}(X)=\\frac{\\alpha}{\\lambda^2},\n\\tag{1.54}\\] \\[\n\\mathbb{E}(X^2)=\\frac{\\alpha+\\alpha^2}{\\lambda^2},\n\\tag{1.55}\\] \\[\n\\mathrm{skew}(X)=\\frac{2}{\\sqrt{\\alpha}},\n\\tag{1.56}\\] \\[\nM_X(t)=\\frac{\\lambda^\\alpha}{(\\lambda-t)^\\alpha}, \\quad t&lt;\\lambda.\n\\tag{1.57}\\]\nNote that \\(\\lambda\\) is called the rate parameter. Often, instead, the scale parameter \\(\\theta=\\frac{1}{\\lambda}\\) is considered.\n\n\n\n\n\n\nviewof a = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\alpha:` })\nviewof l2 = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\lambda:` })\nviewof xmgamma = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction gamma(t,alpha,lambda){\n  return ((lambda**alpha)/mathfn.gamma(alpha))*(t**(alpha-1))*Math.exp(-lambda * t);\n}\n\n\nymaxgamma = d3.max(d3.range(0.1, xmgamma, 0.001).map(t =&gt; gamma(t,a,l2)))\n\n\nPlot.plot({\n  x: { domain: [0.1, xmgamma] },\n  y: { domain: [-0.1*ymaxgamma, ymaxgamma*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0,xmgamma, 0.01)\n        .map(t =&gt; [t, gamma(t,a,l2)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.2: Graph of \\(f(x)=\\dfrac{\\lambda^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1} e^{-\\lambda x}\\)\n\n\n\n\n\n\n\n\n\n1.9 The normal distribution\n\n\n\nA random variable \\(X:\\Omega\\to{\\mathbb{R}}\\) has the normal distribution with the mean \\(\\mu\\) and the variance \\(\\sigma^2\\) if \\[\nf_X(x)=\\frac1{\\sigma\\sqrt{2\\pi}}e^{-\\frac1{2\\sigma^2}(x-\\mu)^2}.\n\\tag{1.58}\\] Indeed, then \\[\n\\mathbb{E}(X)=\\mu, \\quad \\mathrm{Var}(X)=\\sigma^2.\n\\tag{1.59}\\]\nNotation: \\(X\\sim N(\\mu,\\sigma^2)\\).\nObviously, \\(\\mathrm{skew}(X)=0\\).\nThe MGF is defined now everywhere: \\[\nM_X(t)=e^{\\mu t +\\frac12 \\sigma^2 t^2}, \\quad t\\in{\\mathbb{R}}.\n\\tag{1.60}\\]\nFor \\(X\\sim N(0,1)\\), its CDF has special notation: \\[\n  \\Phi(x)=\\int_{-\\infty}^x \\frac1{\\sqrt{2\\pi}}e^{-\\frac1{2}y^2}\\,dy.\n\\tag{1.61}\\]\nFor \\(X\\sim N(\\mu,\\sigma^2)\\), we have then \\[\n  F(x)=\\Phi\\Bigl(\\frac{x-\\mu}{\\sigma}\\Bigr).\n\\tag{1.62}\\]\n\n\n\n\n\n\nviewof mu = Inputs.range([-0.7*xmgauss, 0.7*xmgauss], { value: 0, step: 0.001, label: tex`\\mu:` })\nviewof sigma = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\sigma:` })\nviewof xmgauss = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction gauss(t,mu,sigma){\n  return Math.exp(-(1/(2*sigma**2))*(t-mu)**2)/(sigma*Math.sqrt(2*Math.PI));\n}\n\nymaxgauss = d3.max(d3.range(-xmgauss, xmgauss, 0.01).map(t =&gt; gauss(t,mu,sigma)))\n\nPlot.plot({\n  x: { domain: [-xmgauss, xmgauss] },\n  y: { domain: [-ymaxgauss*0.1, ymaxgauss*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(-xmgauss, xmgauss, 0.01)\n        .map(t =&gt; [t, gauss(t,mu,sigma)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.3: Graph of \\(f_X(x)=\\dfrac1{\\sigma\\sqrt{2\\pi}}e^{-\\frac1{2\\sigma^2}(x-\\mu)^2}\\)\n\n\n\n\n\n\n\n\n\n1.10 The lognormal distribution\n\n\n\nA random variable \\(X:\\Omega\\to{(0,\\infty)}\\) has the lognormal distribution with parameters \\(\\mu\\) and \\(\\sigma^2\\) iff \\[\n\\ln X \\sim N(\\mu,\\sigma^2).\n\\tag{1.63}\\]\nNotation: \\(X\\sim LogN (\\mu,\\sigma^2)\\).\nEquivalently, if \\(Z\\sim N(\\mu,\\sigma^2)\\), then \\(X=e^Z\\).\nMoreover, \\[\n  F_X(x)=F_Z(\\ln x).\n\\tag{1.64}\\]\nTherefore, \\[\nf_X(x)= \\frac{1}{x\\sigma\\sqrt{2\\pi}}\\exp\\biggl(- \\frac{(\\ln x -\\mu)^2}{2\\sigma^2}\\biggr)\n\\tag{1.65}\\]\nFormulas: \\[\n  \\mathbb{E}(X)= e^{\\mu+\\frac12\\sigma^2},\n\\tag{1.66}\\] \\[\n  \\mathrm{Var}(X)=e^{2\\mu+\\sigma^2}\\left( e^{\\sigma^2} -1\\right),\n\\tag{1.67}\\] \\[\n  \\mathrm{skew}(X)=\\left( e^{\\sigma^2} +2\\right)\\sqrt{ e^{\\sigma^2} -1}.\n\\tag{1.68}\\]\n\n\n\n\n\n\nviewof mu2 = Inputs.range([0, xmloggauss*0.5], { value: 0, step: 0.001, label: tex`\\mu:` })\nviewof sigma2 = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\sigma:` })\nviewof xmloggauss = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction loggauss(t,mu,sigma){\n  return Math.exp(-(1/(2*sigma**2))*(Math.log(t)-mu)**2)/(t*sigma*Math.sqrt(2*Math.PI));\n}\n\nymaxloggauss = d3.max(d3.range(0.01, xmloggauss, 0.001).map(t =&gt; loggauss(t,mu2,sigma2)))\n\nPlot.plot({\n  x: { domain: [0.1,xmloggauss] },\n  y: { domain: [-ymaxloggauss*0.1,  ymaxloggauss*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0.1, xmloggauss, 0.01)\n        .map(t =&gt; [t, loggauss(t,mu2,sigma2)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.4: Graph of \\(f_X(x)= \\dfrac{1}{x\\sigma\\sqrt{2\\pi}}\\exp\\biggl(- \\frac{(\\ln x -\\mu)^2}{2\\sigma^2}\\biggr)\\)\n\n\n\n\n\n\n\n\n\n1.11 Pareto distribution\n\n\n\nA random variable \\(X\\) has the Pareto distribution with parameters \\(\\alpha&gt;0\\) and \\(\\lambda&gt;0\\), if \\[\nF_X(x)=1-\\biggl(\\frac{\\lambda}{\\lambda+x}\\biggr)^\\alpha, \\qquad x\\geq0.\n\\tag{1.69}\\]\nNotation: \\(X\\sim Pa(\\alpha,\\lambda)\\).\nThen \\[\nf_X(x)=\\frac{\\alpha\\lambda^\\alpha}{(\\lambda+x)^{\\alpha+1}},\n\\tag{1.70}\\] and, {for \\(\\alpha&gt;1\\)}, \\[\n\\mathbb{E}(X)=\\frac{\\lambda}{\\alpha-1}\n\\tag{1.71}\\]\nA modification of the Pareto distribution is the Burr distribution with an additional parameter \\(\\gamma&gt;0\\) \\[\nF_{Burr}(x)=F_{Pareto}(x^\\gamma)=1-\\biggl(\\frac{\\lambda}{\\lambda+x^\\gamma}\\biggr)^\\alpha.\n\\tag{1.72}\\]\n\n\n\n\n\n\nviewof ap = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`\\alpha:` })\nviewof lp = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`\\lambda:` })\nviewof xmpareto = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction pareto(t,alpha,lambda){\n  return alpha*lambda**alpha*(t+lambda)**(-1-alpha);\n}\n\nymaxpareto = d3.max(d3.range(0.1, xmpareto, 0.01).map(t =&gt; pareto(t,ap,lp)))\n\nPlot.plot({\n  x: { domain: [0.1,xmpareto] },\n  y: { domain: [-ymaxpareto*0.1,  ymaxpareto*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0.1, xmpareto, 0.01)\n        .map(t =&gt; [t, pareto(t,ap,lp)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.5: Graph of \\(f_X(x)= \\dfrac{\\alpha\\lambda^\\alpha}{(\\lambda+x)^{\\alpha+1}}\\)\n\n\n\n\n\n\n\n\n\n1.12 The Weibull distribution\n\n\n\nFor \\(a&gt;0\\), \\(b&gt;0\\), we denote \\(X\\sim W(a,b)\\) iff \\[\nF_X(x)=1-e^{-a x^b},\n\\tag{1.73}\\] \\[\nf_X(x)=ab x^{b-1}e^{-a x^b},\n\\tag{1.74}\\] \\[\n\\mathbb{E}(X)=\\Gamma\\biggl(1+\\frac{1}{b}\\biggr)a^{-\\frac{1}{b}}.\n\\tag{1.75}\\]\n\n\n\n\n\n\nviewof aw = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`a:` })\nviewof bw = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`b:` })\nviewof xmweibull = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction weibull(t,a,b){\n  return a*b*t**(b-1)*Math.exp(-a*t**b);\n}\n\nymaxweibull = d3.max(d3.range(0.1, xmweibull, 0.01).map(t =&gt; weibull(t,aw,bw)))\n\nPlot.plot({\n  x: { domain: [0.1,xmweibull] },\n  y: { domain: [-ymaxweibull*0.1,  ymaxweibull*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0.1, xmweibull, 0.01)\n        .map(t =&gt; [t,  weibull(t,aw,bw)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.6: Graph of \\(f_X(x)= ab x^{b-1}e^{-a x^b}\\)\n\n\n\nyexp = d3.max(d3.range(xmin, xmax, 0.01).map(t =&gt; lexp*Math.exp(-lexp * t)))\nygamma = d3.max(d3.range(xmin, xmax, 0.001).map(t =&gt; gamma(t,agamma,lgamma)))\nygauss = d3.max(d3.range(xmin, xmax, 0.01).map(t =&gt; gauss(t,mu,sigma)))\nyloggauss = d3.max(d3.range(xmin, xmax, 0.001).map(t =&gt; loggauss(t,logmu,logsigma)))\nypareto = d3.max(d3.range(xmin, xmax, 0.01).map(t =&gt; pareto(t,apareto,lpareto)))\nyweibull = d3.max(d3.range(xmin, xmax, 0.01).map(t =&gt; weibull(t,aweibull,bweibull)))\n\ntfull = d3.max([isExp?yexp:0, isGamma?ygamma:0, isGauss?ygauss:0, isPareto?ypareto:0, isWeibull?yweibull:0, isLogGauss?yloggauss:0])\n\nPlot.plot({\n  x: { domain: [xmin, xmax] },\n  y: { domain: [-tfull*0.1,  tfull*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    (isExp == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t =&gt; [t, lexp*Math.exp(-lexp*t)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[0]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t =&gt; [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isGamma == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t =&gt; [t, gamma(t,agamma,lgamma)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[1]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t =&gt; [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isGauss == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t =&gt; [t, gauss(t,muG,sigmaG)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[2]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t =&gt; [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isLogGauss == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t =&gt; [t, loggauss(t,logmu,logsigma)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[3]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t =&gt; [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isPareto == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t =&gt; [t, pareto(t,apareto,lpareto)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[4]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t =&gt; [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isWeibull == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t =&gt; [t,  weibull(t,aweibull,bweibull)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[5]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t =&gt; [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n      Plot.ruleX([xmin]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: xmin })\n  ]\n})\nswatches({\n  color: d3.scaleOrdinal([tex`\\displaystyle f_X(x)=\\lambda e^{-\\lambda x}`, tex`\\displaystyle f_X(x)=\\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1} e^{-\\lambda x}`, tex`\\displaystyle f_X(x)=\\frac1{\\sigma\\sqrt{2\\pi}}e^{-\\frac1{2\\sigma^2}(x-\\mu)^2}`, tex`\\displaystyle f_X(x)= \\frac{1}{x\\sigma\\sqrt{2\\pi}}\\exp\\biggl(- \\frac{(\\ln x -\\mu)^2}{2\\sigma^2}\\biggr)`, tex`\\displaystyle f_X(x)=\\frac{\\alpha\\lambda^\\alpha}{(\\lambda+x)^{\\alpha+1}}`, tex`\\displaystyle f_X(x)=ab x^{b-1}e^{-a x^b}`], d3.schemeCategory10)\n    })\n\n\n\n\n\nimport {legend, swatches} from \"@d3/color-legend\"\n\n\nviewof xmin = Inputs.range([0.1, xmax*0.9], { value: 0.1, step: 0.1, label: tex`x_\\mathrm{min}` })\n\nviewof xmax = Inputs.range([1,1000], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nviewof lexp = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\lambda_{Exp}:` })\n\nviewof agamma = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\alpha_{Gamma}:` })\nviewof lgamma = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\lambda_{Gamma}:` })\n\nviewof muG = Inputs.range([0.1, 10], { value: 0, step: 0.001, label: tex`\\mu_{\\mathcal{N}}:` })\nviewof sigmaG = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\sigma_{\\mathcal{N}}:` })\n\nviewof logmu = Inputs.range([0.1, 10], { value: 0, step: 0.001, label: tex`\\mu_{\\log\\mathcal{N}}:` })\nviewof logsigma = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\sigma_{\\log\\mathcal{N}}:` })\n\nviewof apareto = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`\\alpha_{Pareto}:` })\n\nviewof lpareto = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`\\lambda_{Pareto}:` })\n\nviewof aweibull = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`a_{Weibull}:` })\n\nviewof bweibull = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`b_{Weibull}:` })\nviewof isExp = Inputs.toggle({label: \"exponential\",value: true})\n\nviewof isGamma = Inputs.toggle({label: \"gamma\",value: true})\n\nviewof isGauss = Inputs.toggle({label: \"normal\",value: true})\n\nviewof isLogGauss = Inputs.toggle({label: \"lognormal\",value: true})\n\nviewof isPareto = Inputs.toggle({label: \"Pareto\",value: true})\n\nviewof isWeibull = Inputs.toggle({label: \"Weibull\",value: true})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.7: Comparison of right tails for the density functions",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Loss distributions</span>"
    ]
  },
  {
    "objectID": "Ch-01.html#estimations-of-parameters-from-data",
    "href": "Ch-01.html#estimations-of-parameters-from-data",
    "title": "1  Loss distributions",
    "section": "Estimations of parameters from data",
    "text": "Estimations of parameters from data\n\n\n\n\n\n\n1.13 Likelihood\n\n\n\nLet \\(X:\\Omega\\to{\\mathbb{R}}\\) be a random variable whose distribution depends on a parameter \\(\\theta\\in{\\mathbb{R}}\\).\nSuppose that we observe the data \\(x_1,\\ldots,x_n\\) which is the output of this random variable \\(X\\) in course of \\(n\\) independent trials.\nIn other words, we can say that we observe that i.i.d.r.v. \\(X_1,\\ldots, X_n\\) with \\(X_i\\sim X\\), \\(1\\leq i\\leq n\\), take certain values: \\(X_1=x_1,\\ldots, X_n=x_n\\).\nThe likelihood, or likelihood function, is the function \\(\\mathcal{L}(\\theta)=\\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n)\\) of the unknown parameter \\(\\theta\\) (given the observed data \\(x_1,\\ldots,x_n\\)) which is equal to:\n\n(if \\(X\\) is a discrete random variable) the probability to observe this data (given the value of the parameter \\(\\theta\\)): \\[\n\\begin{aligned}\n\\mathcal{L}(\\theta)&=\\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n)\\\\\n:&=\\mathbb{P}(X_1=x_1,\\ldots,X_n=x_n\\mid \\theta)\n\\\\&= \\mathbb{P}(X_1=x_1\\mid \\theta)\\ldots \\mathbb{P}(X_n=x_n\\mid \\theta).\n\\end{aligned}\n\\tag{1.76}\\]\n(if \\(X\\) is a continuous random variable with the PDF \\(f_X(x)=f_X(x\\mid\\theta)\\)) \\[\n\\begin{aligned}\n\\mathcal{L}(\\theta)&=\\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n)\\\\\n:&=f_X(x_1\\mid\\theta)f_X(x_2\\mid\\theta)\\ldots f_X(x_n\\mid\\theta).\n\\end{aligned}\n\\tag{1.77}\\]\n\n\n\n\n\n\n\n\n\n1.14 Maximum likelihood estimate\n\n\n\nThe maximum likelihood estimate\\(\\theta_*\\) of the parameter \\(\\theta\\) is the argument of the maximum of the likelihood function: \\[\n\\theta_*=\\mathop{\\mathrm{argmax}}_\\theta\\mathcal{L}(\\theta),\n\\tag{1.78}\\] that means that \\[\n\\mathcal{L}(\\theta_*) = \\max_{\\theta}\\mathcal{L}(\\theta).\n\\tag{1.79}\\]\nThe standard approach to find \\(\\theta_*\\) is to consider the log-likelihood function \\[\nL(\\theta):=L(\\theta\\mid x_1,\\ldots,x_n)=\\ln \\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n).\n\\tag{1.80}\\]\nThus, in the discrete case, \\[\nL(\\theta) = \\ln \\mathbb{P}(X_1=x_1\\mid \\theta)+ \\ldots + \\ln \\mathbb{P}(X_n=x_n\\mid \\theta),\n\\tag{1.81}\\] whereas, in the continuous case \\[\nL(\\theta) = \\ln f_X(x_1\\mid\\theta)+ \\ldots + \\ln f_X(x_n\\mid\\theta),\n\\tag{1.82}\\]\nThen \\(\\theta_*\\) is the point of maximum for both \\(\\mathcal{L}\\) and \\(L\\): \\[\n\\theta_*=\\mathop{\\mathrm{argmax}}_\\theta L(\\theta)=\\mathop{\\mathrm{argmax}}_\\theta\\mathcal{L}(\\theta).\n\\tag{1.83}\\]\n\n\n\n\n\n\n\n\n1.15 The Poisson distribution (reminder)\n\n\n\nRecall that \\(X:\\Omega\\to\\mathbb{Z}_+\\) has the Poisson distribution with parameter \\(\\lambda&gt;0\\), notation: \\(X\\sim Po(\\lambda)\\), if \\[\n  \\mathbb{P}(X=n)=\\frac{\\lambda^n}{n!}e^{-\\lambda}, \\qquad n\\in\\mathbb{Z}_+.\n\\tag{1.84}\\] Then \\[\n  \\mathbb{E}(X)=\\mathrm{var}(X)=\\lambda,\n\\tag{1.85}\\] \\[\n  M_X(t)= \\exp\\bigl( \\lambda(e^t-1) \\bigr).\n\\tag{1.86}\\]\n\n\n\n\n\n\n\n\n1.16 Example: Discrete case\n\n\n\nLet \\(X\\sim Po(\\lambda)\\). Suppose we have a sample of \\(n\\) values of \\(X\\): \\(k_1,\\ldots,k_n\\), and we want to estimate \\(\\lambda\\) (here \\(\\theta=\\lambda\\)). Then \\[\n\\begin{aligned}\n\\mathcal{L}(\\lambda)&=\\mathbb{P}(X=k_1\\mid\\lambda)\\ldots \\mathbb{P}(X=k_n\\mid\\lambda)\\\\\n& = \\frac{\\lambda^{k_1}}{k_1!}e^{-\\lambda}\\cdot\\ldots\\cdot \\frac{\\lambda^{k_n}}{k_n!}e^{-\\lambda}\\\\\n& = \\underbrace{\\frac{1}{k_1!\\ldots k_n!}}_{=: c&gt;0}\\lambda^{k_1+\\ldots+k_n}e^{-\\lambda n},\n\\end{aligned}\n\\] and therefore, \\[\n\\begin{aligned}\nL(\\lambda)&=\\ln\\mathcal{L}(\\lambda)\n\\\\& = \\ln c + (k_1+\\ldots+k_n)\\ln\\lambda-\\lambda n.\n\\end{aligned}\n\\] Then \\[\nL'(\\lambda) = \\frac{k_1+\\ldots+k_n}{\\lambda}-n,\n\\] and hence, \\(L'(\\lambda)=0\\) iff \\[\n{\\lambda = \\frac{k_1+\\ldots+k_n}{n}}.\n\\]\nSince \\[\nL''(\\lambda) = (L'(\\lambda))'=-\\frac{k_1+\\ldots+k_n}{\\lambda^2}&lt;0,\n\\] the found value \\(\\lambda_* = \\frac{k_1+\\ldots+k_n}{n}\\) is the point of maximum of \\(L\\), hence, it is the maximum likelihood estimate for the parameter \\(\\lambda\\).\n\n\n\n\n\n\n\n\n1.17 Remark: Relation to the LLN\n\n\n\nNote that, by the law of large numbers (LLN), if \\(X_i\\sim X\\sim Po(\\lambda)\\), \\(i\\in\\mathbb{N}\\), then \\[\n\\frac{X_1+\\ldots+X_n}{n} \\to \\mathbb{E}(X)=\\lambda, \\qquad n\\to\\infty.\n\\] In other words, the maximum likelihood estimate converges to the theoretical value is the size of the sample converges to infinity.\n\n\n\n\n\n\n\n\n1.18 Example: Continuous case\n\n\n\nLet \\(X\\sim Exp(\\lambda)\\). Suppose we have a sample of \\(n\\) values of \\(X\\): \\(x_1,\\ldots,x_n\\). Then \\[\n\\begin{aligned}\n\\mathcal{L}(\\lambda) &= f_X(x_1\\mid \\lambda)\\ldots f_X(x_n\\mid \\lambda)\\\\\n&=\\lambda e^{-\\lambda x_1}\\ldots \\lambda e^{-\\lambda x_n}= \\lambda^n e^{-\\lambda(x_1+\\ldots +x_n)}.\n\\end{aligned}\n\\]\nTherefore, \\[\n\\begin{aligned}\nL(\\lambda)&= \\ln \\mathcal L(\\lambda)\n\\\\&= n \\ln \\lambda - \\lambda(x_1+\\ldots +x_n),\n\\end{aligned}\n\\] so \\(L'(\\lambda)=0\\) iff \\[\n\\begin{gathered}\n\\frac{n}{\\lambda} -(x_1+\\ldots +x_n)=0,\\\\\n{\\lambda_*= \\frac{n}{x_1+\\ldots +x_n}}.\n\\end{gathered}\n\\] Note that \\(L''(\\lambda)=-\\dfrac{n}{\\lambda^2}\\), in particular, \\(L''(\\lambda_*)&lt;0\\), i.e. \\(\\lambda_*\\) is indeed the maximum likelihood estimate for the parameter \\(\\lambda\\).\nFinally, note that if \\(X_i\\sim X\\sim Exp(\\lambda)\\), then, by LLN, \\[\n\\frac{X_1+\\ldots+X_n}{n}\\to \\mathbb{E}(X)=\\frac{1}{\\lambda}, \\qquad n\\to\\infty.\n\\]\n\n\n\n\n\n\n\n\n1.19 The method of moments\n\n\n\nIf the number of parameters is bigger than \\(1\\), the maximum likelihood estimation may be challenging as one would need to find the point of maximum for a function of several variables (though it can be still effectively done numerically).\nAnother method to estimate unknown parameters of the distribution is the method of moments. Namely, assuming that we have a sample of \\(n\\) values \\(x_1,\\ldots,x_n\\) of a random variable \\(X\\) which is believed to be followed a probability distribution which depends on \\(k\\) unknown parameters \\(\\theta=(\\theta_1,\\ldots,\\theta_k)\\).\nThen, we consider the first \\(k\\) moments of the random variable \\(X\\) (i.e. the moments of the population): \\[\nm_j = \\mathbb{E}(X^j\\mid \\theta), \\qquad 1\\leq j\\leq k.\n\\]\nNext, for the given data \\(x_1,\\ldots,x_n\\), we consider \\(k\\) average moments (i.e. the moments of the sample): \\[\n\\overline{m}_j = \\frac{1}{n}\\sum_{l=1}^n x_l ^j, \\qquad 1\\leq j\\leq k.\n\\]\nAfter this, we need to equate \\(k\\) quantities: \\[\nm_j = \\overline{m}_j, \\qquad 1\\leq j\\leq k,\n\\] to determine \\(k\\) unknown parameters \\(\\theta_1,\\ldots,\\theta_k\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Loss distributions</span>"
    ]
  },
  {
    "objectID": "Ch-02.html",
    "href": "Ch-02.html",
    "title": "2  Risk models",
    "section": "",
    "text": "2.1 Definition: Loss\n\n\n\nRecall that (see Definition 1.1), a loss is the value of an individual claim. A risk is a source of potential loss (e.g. a policy or another short-term insurer contract). Each risk may give a rise to a loss (during the period of contract) or it may produce no loss over the period.\n\n\n\n\n\n\n\n\n2.2 Risks and the individual risk model\n\n\n\nSuppose that an insurance company has a portfolio with \\(n\\) policies, hence, \\(n\\) risks: \\(i=1,\\ldots,n\\). Let \\(Y_i\\) represents the total loss from policy \\(i\\) during the period.\nAn individual risk model describes the aggregated loss \\(S\\) from all policies:\n\\[\nS=Y_1+\\ldots+Y_n.\n\\]\nHowever, if policy \\(i\\) didn’t rise a loss, then we should take \\(Y_i=0\\). It is more convenient to rewrite\n\\[\nY_i=J_i \\widetilde{X}_i,\n\\]\nwhere \\(J_i=0\\), if policy \\(i\\) didn’t rise a claim, \\(J_i=1\\) if policy \\(i\\) raised a claim, and then \\(\\widetilde{X}_i\\) is the value of the claim (the loss), distributed randomly according e.g. to one of the loss distributions we studied in the previous chapter.\nTherefore, effectively, \\(S\\) is the sum of those \\(\\widetilde{X}_i\\) where the corresponding \\(J_i=1\\). Consider the total number of policies where claims were raised:\n\\[\nN=\\sum_{i=1}^n J_i.\n\\tag{2.1}\\]\nNow, \\(N\\) is a random number, and \\(S\\) is a sum of \\(N\\) random \\(\\widetilde{X}\\)’s (recall, only whose \\(\\widetilde{X}\\)’s where the corresponding \\(J\\)’s are not zero).\n\n\n\n\n\n\n\n\n2.3 The collective risk model\n\n\n\nTo avoid complicated notations, we rewrite the above as a collective risk model. Namely, let \\(N\\) be the number of claims raised during the time period (i.e. the number of policies which raised claims). We may treat \\(N\\) as a random variable\n\\[\nN:\\Omega\\to\\mathbb{Z}_+:=\\mathbb{N}\\cup\\{0\\}.\n\\]\nAssume that all raised claims, \\(X_1\\), , \\(X_N\\) are i.i.d.r.v. (independent identically distributed random variables), and that \\(N\\) is independent on \\(X_i\\).\nThe aggregated claim (a.k.a. total loss) is then\n\\[\nS=S_N:=X_1+\\ldots+X_N,\n\\tag{2.2}\\]\nwhere \\(S:=0\\) if \\(N=0\\).\n(Note that here \\(X\\)’s are \\(\\widetilde{X}\\)’s with nonzero \\(J\\)’s from the previous subsection!)\nThen\n\\[\nF_S(x)=\\sum_{n=0}^\\infty \\mathbb{P}(N=n)\\, \\mathbb{P}(S_N\\leq x \\mid N=n).\n\\tag{2.3}\\]\nNext,\n\\[\n\\mathbb{E}(S) = \\mathbb{E}(N)\\, \\mathbb{E}(X);\n\\tag{2.4}\\] \\[\n\\mathrm{Var}(S) = \\mathbb{E}(N)\\, \\mathrm{Var}(X) + \\mathrm{Var}(N)\\, \\left( \\mathbb{E}(X) \\right)^2;\n\\tag{2.5}\\] \\[\nM_S(t) = M_N\\left( \\ln M_X(t) \\right).\n\\tag{2.6}\\]\n\n\n\n\n\n\n\n\n2.4 How to model \\(N\\)\n\n\n\nRecall that, in (2.1), \\(J_i\\in\\{0,1\\}\\) is random. Suppose that all \\(J\\)’s are i.i.d.r.v. with\n\\[\n\\mathbb{P}(J=1)=p\\in[0,1].\n\\]\nThen\n\\[\nN=\\sum_{i=1}^n J_i \\sim Bin(n,p),\n\\]\ni.e. \\(N\\) is a binomial random variable with\n\\[\n\\mathbb{P}(N=k)=\\binom{n}{k}p^k (1-p)^{n-k},\n\\tag{2.7}\\]\nwhere \\(k=0,\\ldots, n\\). Note that then \\(\\mathbb{E}(N)=np\\).\nIf \\(n\\) is large (there are a lot of policies), and if \\(p\\) is small (if the probability of a claim per policy were large, an insurer would need to charge very high premiums, and the insurance would no longer be viable), and if, however, \\(\\lambda:=np=\\mathbb{E}(N)\\) is moderate, (so that, in average, there is a non-negligible number of claims), then \\(N\\) can be approximately modelled as a Poisson r.v. with the parameter \\(\\lambda\\):\n\\[\nN\\sim Po(\\lambda).\n\\]\nOf course, the assumption that all \\(J_i\\) have the same distributions, i.e. that all risks (policies) have equal chances to give rise to claims is very restrictive. In particular, for \\(N\\sim Po(\\lambda)\\), we have by (1.85) that \\(\\mathbb{E}(N)=\\mathrm{Var}(N)=\\lambda\\), whereas real insurance data shows that often \\(\\mathrm{Var}(N)&gt;\\mathbb{E}(N)\\).\nInstead, we may assume that probabilities \\(p_i = \\mathbb{P}(J_i=1)\\) are small but different, and consider \\(\\Lambda=\\sum_i p_i\\). It is convenient to model \\(\\Lambda\\) as a random variable itself. In particular, if \\(\\Lambda\\sim \\Gamma(k,\\beta)\\) for some \\(k\\in\\mathbb{N}\\), \\(\\beta&gt;0\\), then \\(N\\) has negative binomial distribution with parameters \\(k\\) and \\(p:=\\frac{\\beta}{1+\\beta}\\in(0,1)\\), i.e.~\\(N\\sim NB(k,p)\\) with \\[\n\\mathbb{P}(N=n)=\\binom{n+k-1}{n}p^k(1-p)^n, \\quad n\\in\\mathbb{Z}_+.\n\\tag{2.8}\\]\n\n\n\n\n\n\n\n\n2.5 The compound Poisson distribution (cPd)\n\n\n\nLet \\(N\\sim Po(\\lambda)\\), i.e. for \\(n\\in\\mathbb{Z}_+\\),\n\\[\n\\mathbb{P}(N=n)=\\frac{\\lambda^n}{n!}e^{-\\lambda}, \\qquad n\\in\\mathbb{Z}_+.\n\\]\nLet \\(X_1, \\ldots, X_N\\) be i.i.d.r.v. Then \\(S=X_1+\\ldots+X_N\\) is said to have the compound Poisson distribution (cPd) with the parameter \\(\\lambda\\). Formulas: \\[\n\\mathbb{E}(S)=\\lambda m_{1,X},\n\\tag{2.9}\\] \\[\n\\mathrm{Var}(S)=\\lambda m_{2,X},\n\\tag{2.10}\\] \\[\n\\mathrm{skew}(S)=\\lambda m_{3,X},\n\\tag{2.11}\\] \\[\nM_S(t)= \\exp\\bigl( \\lambda (M_X(t)-1) \\bigr).\n\\tag{2.12}\\]\nAn important property is that a sum of independent compound Poisson r.v. is again a compound Poisson r.v. Namely, let \\(S_1,\\ldots, S_m\\) be independent compound Poisson random variables with parameters \\(\\lambda_i\\) and CDF of the single claims \\(F_i(x)\\), \\(1\\leq i\\leq m\\), i.e.\n\\[\nS_i = X_1^{(i)}+\\ldots+X_{N_i}^{(i)},\n\\]\nwhere, for each \\(i\\), \\(\\{X_j^{(i)}\\}\\) are i.i.d.r.v. with CDF \\(F_i\\), and\n\\[\nN_i\\sim Po(\\lambda_i).\n\\]\nMoreover, let \\(\\{X_j^{(i)}, N_i\\}\\) be also mutually independent for different \\(i\\).\nThen\n\\[\n\\mathbf{S}:=S_1+\\ldots+S_m\n\\]\nis a compound Poisson r.v. with the parameter\n\\[\n\\Lambda=\\lambda_1+\\ldots+\\lambda_m\n\\tag{2.13}\\]\nand the CDF of the single claim is\n\\[\nF(x)=\\frac{1}{\\Lambda} \\bigl( \\lambda_1 F_1(x)+\\ldots+\\lambda_m F_m(x) \\bigr),\n\\tag{2.14}\\]\ni.e. \\(S=Y_1+\\ldots+Y_N\\) for (some) i.i.d.r.v. \\(\\{Y_j\\}\\) with the CDF \\(F(x)\\), and \\(N\\sim Po(\\Lambda)\\).\n\n\n\n\n\n\n\n\n2.6 The compound binomial distribution\n\n\n\nLet \\(N\\sim Bin(n,p)\\), \\(n\\in\\mathbb{N}\\), \\(p\\in[0,1]\\), i.e. (2.7) holds, and also \\(\\mathbb{E}(N) = np\\), \\(\\mathrm{Var}(N)=np(1-p)\\), \\(M_N(t)=(pe^t+1-p)^n\\). Let \\(X_1, \\ldots, X_N\\) be i.i.d.r.v. Then \\(S=X_1+\\ldots+X_N\\) is said to have the compound binomial distribution. Formulas:\n\\[\n\\mathbb{E}(S)=np m_{1,X},\n\\tag{2.15}\\] \\[\n\\mathrm{Var}(S)= np m_{2,X} -np^2 m_{1,X}^2,\n\\tag{2.16}\\] \\[\nM_S(t) = (pM_X(t)+1-p)^n,\n\\tag{2.17}\\] \\[\n\\mathrm{skew}(S)=np m_{3,X}-3np^2m_{2,X}m_{1,X} +2np^3m_{1,X}^3.\n\\tag{2.18}\\]\n\n\n\n\n\n\n\n\n2.7 The compound negative binomial distribution\n\n\n\nLet \\(N\\sim NB(k,p)\\), \\(k\\in\\mathbb{N}\\), \\(p\\in[0,1]\\), i.e. (2.8) holds. Then, for \\(q=1-p\\), \\(\\mathbb{E}(N)=\\frac{kq}{p}\\), \\(\\mathrm{Var}(N)=\\frac{kq}{p^2}\\), \\(M_N(t)=p^k(1-qe^t)^{-k}\\). Let \\(X_1, \\ldots, X_N\\) be i.i.d.r.v. Then \\(S=X_1+\\ldots+X_N\\) is said to have the compound negative binomial distribution. Formulas: \\[\n\\mathbb{E}(S)=\\frac{kq}{p}m_{1,X},\n\\tag{2.19}\\] \\[\n\\mathrm{Var}(S)=\\frac{kq}{p}m_{2,X}+\\frac{kq^2}{p^2}m_{1,X}^2,\n\\tag{2.20}\\] \\[\nM_S(t)=\\frac{p^k}{(1-qM_X(t))^k},\n\\tag{2.21}\\] \\[\n\\mathrm{skew}(S)=\\frac{3kq^2m_{1,X}m_{2,X}}{p^2}+\\frac{2kq^3m_{1,X}^3}{p^3}+\\frac{kqm_{3,X}}{p}.\n\\tag{2.22}\\]\n\n\n\n\n\n\n\n\n2.8 Reinsurance\n\n\n\n\n\\(X\\) is the gross claim amount random variable\n\\(Y\\) is the net claim amount (paid by the main insurer after receiving the reinsurance recovery)\n\\(Z\\) is the amount paid by the reinsurer\n\n\n\n\n\n\n\n\n\n2.9 Example: Proportional reinsurance\n\n\n\nFor \\(0&lt;\\alpha&lt;1\\), we set\n\\[\nY=\\alpha X, \\qquad Z=(1-\\alpha)X.\n\\tag{2.23}\\]\nThen\n\\[\n    \\begin{alignat*}{2}\n        \\mathbb{E}(Y)&=\\alpha\\mathbb{E}(X), &\\qquad\n        \\mathbb{E}(Z)&=(1-\\alpha)\\mathbb{E}(X),\\\\\n        \\mathrm{Var}(Y)&=\\alpha^2\\mathrm{Var}(X), &\\qquad\n        \\mathrm{Var}(Z)&=(1-\\alpha)^2\\mathrm{Var}(X).\n    \\end{alignat*}\n\\]\n\n\n\n\n\n\n\n\n2.10 Example: Excess of loss reinsurance\n\n\n\nThe insurer pays in full up to an amount \\(M\\), the reinsurer pays all above \\(M\\), if needed:\n\\[\nY=\\begin{cases}\nX, & \\text{if } X\\leq M,\\\\\nM, & \\text{if } X&gt;M;\n\\end{cases} \\qquad Z=X-Y.\n\\tag{2.24}\\]\nThen \\[\nm_{k,Y} = \\int_0^M x^k f_X(x)\\,dx + M^k \\mathbb{P}(X&gt;M),\n\\tag{2.25}\\] \\[\nM_Y(t)= \\int_0^M e^{tx}f_X(x)\\,dx +e^{tM}\\mathbb{P}(X&gt;M),\n\\tag{2.26}\\] \\[\nm_{k,Z} = \\int_M^\\infty (x-M)^k f_X(x)\\,dx,\n\\tag{2.27}\\] \\[\nM_Z(t) = \\mathbb{P}(X\\leq M) +\\int_M^\\infty e^{t(x-M)}f_X(x)\\,dx.\n\\tag{2.28}\\]\n\n\n\n\n\n\n\n\n2.11 Reinsurer’s view\n\n\n\nIn the conditions of the previous example, the reinsurer pays \\(Z\\), evidently, only if \\(Z=X-Y&gt;0\\), otherwise, the reinsurer even does not know that a claim happened.\nHence, the distribution function for reinsurer is\n\\[\n\\begin{align}\nF_Z(z)&=\\mathbb{P}(Z\\leq z\\mid Z&gt;0)=\\mathbb{P}(X\\leq z+M\\mid X&gt;M)\\\\\n&= \\frac{\\mathbb{P}(M&lt;X\\leq z+M)}{\\mathbb{P}(X&gt;M)}= \\frac{F_X(z+M)-F_X(M)}{1-F_X(M)}.\n\\end{align}\n\\]\nThen\n\\[\nf_Z(z)= \\frac{f_X(z+M)}{1-F_X(M)}, \\quad z&gt;0.\n\\]\n\n\n\n\n\n\n\n\n2.12 Agrregated claims with reinsurance\n\n\n\n\nThe aggregated claim \\(S=X_1+\\ldots+X_N\\) of a collective risk model can be done when the reinsurance is in force.\nConsider the case when the reinsurance is applied to each individual claim.\nLet \\(Y_i\\) be the amount paid by the main insurer for the claim \\(X_i\\), and \\(Z_i\\) be the claim paid by the reinsurer, so \\(X_i=Y_i+Z_i\\).\nLet \\(S_I=Y_1+\\ldots+Y_N\\) be the aggregated claim amount paid by the insurer, and \\(S_R=Z_1+\\ldots+Z_N\\) be the aggregated claim amount paid by the reinsurer.\nOf course, formulas (2.4)–(2.6) are still true if we replace there \\(S,X\\) by \\(S_I,Y\\) or by \\(S_R,Z\\), respectively.\nNote that, for the individual excess of loss reinsurance, the reinsurer pays \\(Z_i\\) only if \\(Z_i=X_i-M&gt;0\\). We can introduce \\(J_i=1\\) if \\(X_i&gt;M\\) and \\(J_i=0\\) otherwise. Then \\(N_R:=J_1+\\ldots+J_N\\) is the number of claims paid by the reinsurer.\n\\(X_1,\\ldots,X_N\\) are i.i.d.r.v., let\n\\[\n  \\mathbb{P}(J_i=1)=\\mathbb{P}(X_i&gt;M)=:\\rho.\n  \\]\nThen (actually \\(J_i\\sim Bin(1,\\rho)\\))\n\\[\n  M_{J_i}(t)=\\mathbb{E}(e^{t J_i})= \\rho e^t +(1-p)e^0 = \\rho e^t +1-p,\n  \\]\nand for the compound random variable \\(N_R\\),\n\\[\n  M_{N_R}(t)=M_N\\bigl(\\ln (\\rho e^t +1-p)\\bigr).\n   \\tag{2.29}\\]\nLet \\(\\{i_1,\\ldots,i_{N_R}\\}\\subset\\{1,\\ldots, N\\}\\) be indexes such that \\(X_{i_k}&gt;M\\) for \\(1\\leq k\\leq N_R\\), and set \\(W_k:=X_{i_k}-M\\). Then there is another representation\n\\[\n  S_R = W_1+\\ldots + W_{N_R}.\n   \\tag{2.30}\\]\nIf, additionally, \\(N\\sim Po(\\lambda)\\) for a \\(\\lambda&gt;0\\), then (2.29) reads, by (1.86) or (2.12),\n\\[\n  M_{N_R}(t) =\\exp\\Bigl(\\lambda \\bigl(M_{J_i}(t)-1\\bigr)\\Bigr) =\\exp\\bigl(\\rho\\lambda (e^t-1)\\bigr),\n  \\]\ni.e. \\(N_R\\sim Po(\\rho\\lambda)\\), by (1.86) (the thinning property of the Poisson distribution.)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Risk models</span>"
    ]
  }
]