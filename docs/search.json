[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA-365/M65 Lecture Notes",
    "section": "",
    "text": "Overview\nUse the left panel to navigate the website.\nUse search field on the left to find all instances of a term in the Lecture Notes.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "Ch-01.html",
    "href": "Ch-01.html",
    "title": "1  Loss distributions",
    "section": "",
    "text": "mathfn = require('https://bundle.run/mathfn@1.1.0')\n\n\n\n\n\n\n\n\n\n\n\n\nCaution1.1 Definition: Loss\n\n\n\nLoss is the value of an individual claim. It is modelled as a random variable.\n\n\n\n\n\n\n\n\nNote1.2 Probability distribution\n\n\n\n\nLet \\((\\Omega,\\mathcal{A},\\mathbb{P})\\) be a probability space.\nRecall that \\[\n\\mathbb{P}(A)\\in[0,1], \\quad A\\in \\mathcal{A};\n\\tag{1.1}\\] \\[\n\\mathbb{P}(\\emptyset)=0, \\qquad \\mathbb{P}(\\Omega)=1.\n\\tag{1.2}\\]\nA measurable function \\(X:\\Omega\\to\\mathbb{R}\\) is called a random variable.\nIf \\(X\\) can take any value from an uncountable set, e.g. an interval of \\(\\mathbb{R}\\), \\(X\\) is called a continuous random variable.\nThe cummulative distribution function (CDF) of \\(X\\) is a non-decreasing function \\[\nF_X(x)=\\mathbb{P}(X\\leq x)\\in[0,1].\n\\tag{1.3}\\]\nProperties: \\[\n  \\lim_{x\\to-\\infty}F_X(x)=0,\n\\tag{1.4}\\] \\[\n  \\lim_{x\\to+\\infty}F_X(x)=1;\n\\tag{1.5}\\] \\[\n  \\mathbb{P}(a\\leq X \\leq b) = F_X(b)-F_X(a).\n\\tag{1.6}\\]\nThe survival function \\(S_X\\) is given by: \\[\n  S_X(x):=\\mathbb{P}(X&gt;x)=1-F_X(x).\n\\tag{1.7}\\]\nIf \\(F_X\\) is differentiable (at almost all \\(x\\in\\mathbb{R}\\)), \\(X\\) is called an absolutely continuous random variable. In this case, the derivative of \\(F_X\\) is called the probability density function (PDF) (a.k.a. probability density) of \\(X\\): \\[\nf_X(x)=F'_X(x)\\geq0.\n\\tag{1.8}\\]\nRelations: \\[\n  F_X(x)=\\int_{-\\infty}^x f_X(x)\\,dx,\n\\tag{1.9}\\] \\[\n  \\mathbb{P}(a\\leq X \\leq b) = \\int_a^b f_X(x)\\,dx,\n\\tag{1.10}\\] \\[\n  S_X(x)=\\int_x^{\\infty} f_X(x)\\,dx,\n\\tag{1.11}\\] \\[\n  \\int_{-\\infty}^\\infty f_X(x)\\,dx =1.\n\\tag{1.12}\\]\nWe consider only absolutely continuous random variables, so we omit ``absolutely’’.\nNote that, if \\(X\\) is a continuous random variable then \\[\n  \\mathbb{P}(X=a)=0\\qquad \\text{for each } a\\in\\mathbb{R}.\n\\]\n\n\n\n\n\n\n\n\n\nNote1.3 Moments\n\n\n\n\nFor any (measurable) \\(g:\\mathbb{R}\\to\\mathbb{R}\\), \\(g(X):\\Omega\\to\\mathbb{R}\\) is also a random variable. The expectation (a.k.a. mean) of \\(g(X)\\) is \\[\n\\mathbb{E}(g(X)) = \\int_{\\mathbb{R}} g(x) f_X(x)\\,dx.\n   \\tag{1.13}\\]\nFor a \\(k\\in\\mathbb{N}\\), the moment of order \\(k\\) of \\(X\\) is \\[\nm_k=m_{k,X}=\\mathbb{E}(X^k)=\\int_{\\mathbb{R}} x^k \\, f_X(x)\\,dx.\n\\tag{1.14}\\]\nIn particular, the expectation of \\(X\\) is its first moment: \\[\n\\mathbb{E}(X) = m_{1,X}=\\int_{\\mathbb{R}} x \\, f_X(x)\\,dx.\n\\tag{1.15}\\] It measures the average value of \\(X\\) (over many trials).\nRemember that, for \\(X,Y:\\Omega\\to\\mathbb{R}\\), \\(\\alpha\\in\\mathbb{R}\\), \\[\n  \\mathbb{E}(\\alpha X) = \\alpha \\, \\mathbb{E}(X),\n\\tag{1.16}\\] \\[\n  \\mathbb{E}(X+Y)=\\mathbb{E}(X)+\\mathbb{E}(Y).\n\\tag{1.17}\\]\nThe central moment of order \\(k\\) of \\(X\\) is \\[\n  \\begin{aligned}\n    \\widetilde{m}_k&=\\widetilde{m}_{k,X}=\\mathbb{E}\\bigl(X-\\mathbb{E}(X)\\bigr)^k\\notag\\\\&=\\int_\\mathbb{R} \\bigl(x - \\mathbb{E}(X)\\bigr)^k \\, f_X(x)\\,dx.\n  \\end{aligned}\n  \\tag{1.18}\\]\nThe variance of a random variable \\(X\\) is its second central moment: \\[\n\\begin{aligned}\n\\mathrm{Var}(X):&= \\mathbb{E}\\bigl((X-\\mathbb{E}(X))^2\\bigr)\\\\\n            & = \\mathbb{E}(X^2)-\\bigl(\\mathbb{E}(X)\\bigr)^2=m_{2,X}-(m_{1,X})^2\n\\end{aligned}\n\\tag{1.19}\\] It measures the spread or dispersion of the values of \\(X\\) (over many trials). Note that \\[\n  \\mathrm{Var}(\\alpha X) = \\alpha^2 \\mathrm{Var} (X), \\quad \\alpha\\in\\mathbb{R}.\n\\tag{1.20}\\]\nThe skewness of \\(X\\) is its third central moment: \\[\n  \\mathrm{skew}(X):= \\mathbb{E}\\bigl((X-\\mathbb{E}(X))^3\\bigr).\n\\tag{1.21}\\] It measures the asymmetry of the distribution of \\(X\\) about its mean. Note that the negative skew means that the left tail of \\(f_X(x)\\) is ``longer’’, i.e. the graph of \\(f_X(x)\\) is inclined to the right; whereas positive skew means that the graph of \\(f_X(x)\\) is inclined to the left.\nThe standard deviation of \\(X\\) is \\[\n  \\sigma(X):=\\sqrt{ \\mathrm{Var}(X)}\\geq0.\n\\tag{1.22}\\]\nThe coefficient of skewness of \\(X\\) is \\[\n  \\gamma_X := \\frac{\\widetilde{m}_{3,X}}{\\sigma(X)^3}=\\mathbb{E}\\biggl(\\Bigl(\\frac{X-\\mathbb{E}(X)}{\\sigma(X)}\\Bigr)^3\\biggr)\\in\\mathbb{R}.\n\\tag{1.23}\\]\nThe moment generator function (MGM) is \\[\nM_X(t)=\\mathbb{E}(e^{tX})=\\int_{\\mathbb{R}} e^{tx} \\, f_X(x)\\,dx\\geq0.\n\\tag{1.24}\\]\nRelation: \\[\nM_X(t)=1+\\sum_{k=1}^\\infty\\frac{m_k}{k!}t^k.\n\\tag{1.25}\\] and \\[\nm_{k,X}  = M_X^{(k)}(0)\n\\tag{1.26}\\]\nA random variable is uniquely determined by either of CDF, PDF, MGF.\n\n\n\n\n\n\n\n\n\nNote1.4 Conditional characteristics\n\n\n\n\nFor random variables \\(X\\) and \\(Y\\), the \\(\\mathbb{E}(X\\mid Y)\\) (of \\(X\\) given \\(Y\\)) is a random variable measurable with respect to the \\(\\sigma\\)-algebra generated by \\(Y\\) (i.e. by all subsets \\(\\{Y\\leq a\\}\\subset\\Omega\\) for \\(a\\in\\mathbb{R}\\)) such that \\[\n\\mathbb{E}\\bigl(\\mathbb{E}(X\\mid Y)\\bigr) = \\mathbb{E}(X).\n\\tag{1.27}\\]\nIn particular, if \\(Y_1,\\ldots,Y_n\\) makes a partition of \\(\\Omega\\) (i.e. their disjoint union is \\(\\Omega\\)), then \\[\n\\mathbb{E}(X)=\\sum_{i=1}^n \\mathbb{E}(X\\mid Y_i)\\mathbb{P}(Y_i).\n\\tag{1.28}\\]\nThe conditional variance of \\(X\\) given \\(Y\\) is the random variable \\[\n\\mathrm{Var}(X\\mid Y) = \\mathbb{E}\\Bigl(\n      \\bigl(X- \\mathbb{E}(X\\mid Y)\\bigr)^2 \\Bigm\\vert Y \\Bigr)\n\\tag{1.29}\\]\nThe law of total variance is that \\[\n\\mathrm{Var}(X) = \\mathbb{E}\\bigl(\\mathrm{Var}(X\\mid Y)\\bigr) + \\mathrm{Var}\\bigl(\\mathbb{E}(X\\mid Y)\\bigr).\n\\tag{1.30}\\]\n\n\n\n\n\n\n\n\n\nNote1.5 Covariance and correlation\n\n\n\n\nFor random variables \\(X,Y:\\Omega\\to\\mathbb{R}\\), their covariance is defined by \\[    \n  \\begin{aligned}\n    \\mathrm{cov}(X,Y)&=\\mathbb{E}\\bigl((X-\\mathbb{E}(X))(Y-\\mathbb{E}(Y))\\bigr)\\\\\n    &=\\mathbb{E}(XY)-\\mathbb{E}(X)\\mathbb{E}(Y).\n  \\end{aligned}\n\\tag{1.31}\\]\nProperties: \\[\n  \\mathrm{cov}(X,X)=\\mathrm{Var}(X),\n\\tag{1.32}\\] \\[\n\\mathrm{cov}(\\alpha X, \\beta Y)=\\alpha\\beta \\mathrm{cov}(X,Y),\n\\tag{1.33}\\] \\[\n  \\begin{aligned}\n  \\mathrm{cov}(X_1+X_2,Y_1+Y_2)&=\\mathrm{cov}(X_1,Y_1)+\\mathrm{cov}(X_1,Y_2)\\notag\\\\&\\quad+\\mathrm{cov}(X_2,Y_1)+\\mathrm{cov}(X_2,Y_2).\n  \\end{aligned}\n\\]\nThe correlation of \\(X\\) and \\(Y\\) is defined by \\[\n   \\mathrm{corr}(X,Y)=\\frac{\\mathrm{cov}(X,Y)}{\\sigma(X)\\sigma(Y)}\\in[-1,1].\n\\tag{1.34}\\]\nTwo random variables \\(X\\) and \\(Y\\) are called uncorrelated if \\(\\mathrm{cov}(X,Y)=0\\).\nIn particular, if \\(X\\) and \\(Y\\) are independent then \\(\\mathbb{E}(XY)=\\mathbb{E}(X)\\mathbb{E}(Y)\\), and hence they are uncorrelated. The opposite, however, is not necessary true.\nIf \\(X\\) and \\(Y\\) are uncorrelated then \\[\n  \\mathrm{Var}(X+Y)=\\mathrm{Var}(X)+\\mathrm{Var}(Y).\n\\tag{1.35}\\]\nFor general \\(X\\) and \\(Y\\), \\[\n  \\mathrm{Var}(X+Y)=\\mathrm{Var}(X)+\\mathrm{Var}(Y)+2\\mathrm{cov}(X,Y).\n\\]\n\n\n\n\n\n\n\n\n\nNote1.6 The exponential distribution\n\n\n\nA random variable \\(X:\\Omega\\to{\\mathbb{R}}\\) has the exponential distribution with parameter \\(\\lambda&gt;0\\) if its CDF is \\[\nF(x)=1-e^{-\\lambda x}, \\quad x\\geq0.\n\\tag{1.36}\\]\nHere and below, when we write a restriction on \\(x\\), we mean that function is \\(0\\) otherwise, i.e. here \\(F(x)=0\\) for \\(x&lt;0\\).\nThen the PDF is \\[\nf(x)=\\lambda e^{-\\lambda x}, \\quad x\\geq0.\n\\tag{1.37}\\]\nNext, \\[\n\\mathbb{E}(X)=\\frac1{\\lambda},\n\\tag{1.38}\\] \\[\n\\mathrm{Var}(X)=\\frac1{\\lambda^2}= \\bigl( \\mathbb{E}(X) \\bigr)^2,\n\\tag{1.39}\\] \\[\n\\mathbb{E}(X^2)=\\frac2{\\lambda^2}=2\\bigl( \\mathbb{E}(X) \\bigr)^2.\n\\tag{1.40}\\] The MGM if defined for \\(t&lt;\\lambda\\) only: \\[\nM(t)=\\biggl(1-\\frac{t}{\\lambda}\\biggr)^{-1}=\\frac{\\lambda}{\\lambda-t}.\n\\] The notation is \\[\nX\\sim Exp(\\lambda).\n\\tag{1.41}\\]\n\n\n\n\n\n\nviewof l = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\lambda:` })\n\nviewof xmexp = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nymaxexp = d3.max(d3.range(0, xmexp, 0.01).map(t =&gt; l*Math.exp(-l * t)))\n\nPlot.plot({\n  x: { domain: [-0.1, xmexp] },\n  y: { domain: [-0.1*ymaxexp, ymaxexp*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0, xmexp, 0.1)\n        .map(t =&gt; [\n          t,\n          l*Math.exp(-l * t)\n        ]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.1: Graph of \\(f(x)=\\lambda e^{-\\lambda x}\\)\n\n\n\n\n\n\n\n\n\nNote1.7 The gamma distribution\n\n\n\nA random variable \\(X:\\Omega\\to{\\mathbb{R}}\\) has the gamma distribution with parameters \\(\\alpha&gt;0\\) and \\(\\lambda&gt;0\\) if its PDF is \\[\nf(x)=\\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1} e^{-\\lambda x}, \\quad x\\geq0.\n\\tag{1.42}\\] Here \\[\n\\Gamma(\\alpha):=\\int_0^\\infty t^{\\alpha-1}e^{-t}\\,dt, \\quad \\alpha&gt;0\n\\tag{1.43}\\] is the gamma function.\nThe following formulas hold: \\[\n\\mathbb{E}(X)=\\frac{\\alpha}{\\lambda},\n\\tag{1.44}\\] \\[\n\\mathbb{E}(X^2)=\\frac{\\alpha+\\alpha^2}{\\lambda^2},\n\\tag{1.45}\\] \\[\n\\mathrm{Var}(X)=\\frac{\\alpha}{\\lambda^2},\n\\tag{1.46}\\] \\[\nM_X(t)=\\biggl(1-\\frac{t}{\\lambda}\\biggr)^{-\\alpha}=\\frac{\\lambda^\\alpha}{(\\lambda-t)^\\alpha}, \\quad t&lt;\\lambda.\n\\tag{1.47}\\] The notation is \\[\nX\\sim \\Gamma(\\alpha,\\lambda).\n\\]\n\n\n\n\n\n\nviewof a = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\alpha:` })\nviewof l2 = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\lambda:` })\nviewof xmgamma = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction gamma(t,alpha,lambda){\n  return ((lambda**alpha)/mathfn.gamma(alpha))*(t**(alpha-1))*Math.exp(-lambda * t);\n}\n\n\nymaxgamma = d3.max(d3.range(0.1, xmgamma, 0.001).map(t =&gt; gamma(t,a,l2)))\n\n\nPlot.plot({\n  x: { domain: [0.1, xmgamma] },\n  y: { domain: [-0.1*ymaxgamma, ymaxgamma*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0,xmgamma, 0.01)\n        .map(t =&gt; [t, gamma(t,a,l2)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.2: Graph of \\(f(x)=\\dfrac{\\lambda^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1} e^{-\\lambda x}\\)\n\n\n\n\n\n\n\n\n\nNote1.8 The normal distribution\n\n\n\nA random variable \\(X:\\Omega\\to{\\mathbb{R}}\\) has the normal distribution with the mean \\(\\mu\\) and the variance \\(\\sigma^2\\) if \\[\nf_X(x)=\\frac1{\\sigma\\sqrt{2\\pi}}e^{-\\frac1{2\\sigma^2}(x-\\mu)^2}.\n\\tag{1.48}\\] Indeed, \\[\n\\mathbb{E}(X)=\\mu, \\quad \\mathrm{Var}(X)=\\sigma^2.\n\\tag{1.49}\\] The MGF is defined now everywhere: \\[\nM_X(t)=e^{\\mu t +\\frac12 \\sigma^2 t^2}, \\quad t\\in{\\mathbb{R}}.\n\\tag{1.50}\\] Notation \\[\nX\\sim N(\\mu,\\sigma^2).\n\\]\n\n\n\n\n\n\nviewof mu = Inputs.range([-0.7*xmgauss, 0.7*xmgauss], { value: 0, step: 0.001, label: tex`\\mu:` })\nviewof sigma = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\sigma:` })\nviewof xmgauss = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction gauss(t,mu,sigma){\n  return Math.exp(-(1/(2*sigma**2))*(t-mu)**2)/(sigma*Math.sqrt(2*Math.PI));\n}\n\nymaxgauss = d3.max(d3.range(-xmgauss, xmgauss, 0.01).map(t =&gt; gauss(t,mu,sigma)))\n\nPlot.plot({\n  x: { domain: [-xmgauss, xmgauss] },\n  y: { domain: [-ymaxgauss*0.1, ymaxgauss*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(-xmgauss, xmgauss, 0.01)\n        .map(t =&gt; [t, gauss(t,mu,sigma)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.3: Graph of \\(f_X(x)=\\dfrac1{\\sigma\\sqrt{2\\pi}}e^{-\\frac1{2\\sigma^2}(x-\\mu)^2}\\)\n\n\n\n\n\n\n\n\n\nNote1.9 The lognormal distribution\n\n\n\nA random variable \\(X:\\Omega\\to{(0,\\infty)}\\) has the lognormal distribution with parameters \\(\\mu\\) and \\(\\sigma^2\\) iff \\[\n\\ln X \\sim N(\\mu,\\sigma^2).\n\\tag{1.51}\\] Equivalently, if \\(Z\\sim N(\\mu,\\sigma^2)\\), then \\(X=e^Z\\).\nNotation: \\[\nX\\sim\\ln N (\\mu,\\sigma^2).\n\\]\nThen \\[\n\\mathbb{E}(X)= e^{\\mu+\\frac12\\sigma^2},\n\\tag{1.52}\\] \\[\n\\mathrm{Var}(X)=e^{2\\mu+\\sigma^2}\\left( e^{\\sigma^2} -1\\right),\n\\tag{1.53}\\] \\[\nf_X(x)= \\frac{1}{x\\sigma\\sqrt{2\\pi}}\\exp\\biggl(- \\frac{(\\ln x -\\mu)^2}{2\\sigma^2}\\biggr)\n\\tag{1.54}\\]\n\n\n\n\n\n\nviewof mu2 = Inputs.range([0, xmloggauss*0.5], { value: 0, step: 0.001, label: tex`\\mu:` })\nviewof sigma2 = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\sigma:` })\nviewof xmloggauss = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction loggauss(t,mu,sigma){\n  return Math.exp(-(1/(2*sigma**2))*(Math.log(t)-mu)**2)/(t*sigma*Math.sqrt(2*Math.PI));\n}\n\nymaxloggauss = d3.max(d3.range(0.01, xmloggauss, 0.001).map(t =&gt; loggauss(t,mu2,sigma2)))\n\nPlot.plot({\n  x: { domain: [0.1,xmloggauss] },\n  y: { domain: [-ymaxloggauss*0.1,  ymaxloggauss*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0.1, xmloggauss, 0.01)\n        .map(t =&gt; [t, loggauss(t,mu2,sigma2)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.4: Graph of \\(f_X(x)= \\dfrac{1}{x\\sigma\\sqrt{2\\pi}}\\exp\\biggl(- \\frac{(\\ln x -\\mu)^2}{2\\sigma^2}\\biggr)\\)\n\n\n\n\n\n\n\n\n\nNote1.10 Pareto distribution\n\n\n\nA random variable \\(X\\) has the two-parameter Pareto distribution with parameters \\(\\alpha&gt;0\\) and \\(\\lambda&gt;0\\), if \\[\nF_X(x)=1-\\biggl(\\frac{\\lambda}{\\lambda+x}\\biggr)^\\alpha, \\qquad x\\geq0.\n\\tag{1.55}\\] Notation \\(X\\sim Pa(\\alpha,\\lambda)\\).\nThen \\[\nf_X(x)=\\frac{\\alpha\\lambda^\\alpha}{(\\lambda+x)^{\\alpha+1}},\n\\tag{1.56}\\] and, {for \\(\\alpha&gt;1\\)}, \\[\n\\mathbb{E}(X)=\\frac{\\lambda}{\\alpha-1}\n\\tag{1.57}\\] A modification of the Pareto distribution is the Burr distribution with additional parameter \\(\\gamma&gt;0\\) \\[\nF_{Burr}(x)=F_{Pareto}(x^\\gamma)=1-\\biggl(\\frac{\\lambda}{\\lambda+x^\\gamma}\\biggr)^\\alpha.\n\\tag{1.58}\\]\n\n\n\n\n\n\nviewof ap = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`\\alpha:` })\nviewof lp = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`\\lambda:` })\nviewof xmpareto = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction pareto(t,alpha,lambda){\n  return alpha*lambda**alpha*(t+lambda)**(-1-alpha);\n}\n\nymaxpareto = d3.max(d3.range(0.1, xmpareto, 0.01).map(t =&gt; pareto(t,ap,lp)))\n\nPlot.plot({\n  x: { domain: [0.1,xmpareto] },\n  y: { domain: [-ymaxpareto*0.1,  ymaxpareto*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0.1, xmpareto, 0.01)\n        .map(t =&gt; [t, pareto(t,ap,lp)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.5: Graph of \\(f_X(x)= \\dfrac{\\alpha\\lambda^\\alpha}{(\\lambda+x)^{\\alpha+1}}\\)\n\n\n\n\n\n\n\n\n\nNote1.11 The Weibull distribution\n\n\n\nFor \\(a&gt;0\\), \\(b&gt;0\\), we denote \\(X\\sim W(a,b)\\) iff \\[\nF_X(x)=1-e^{-a x^b},\n\\tag{1.59}\\] \\[\nf_X(x)=ab x^{b-1}e^{-a x^b},\n\\tag{1.60}\\] \\[\n\\mathbb{E}(X)=\\Gamma\\biggl(1+\\frac{1}{b}\\biggr)a^{-\\frac{1}{b}}.\n\\tag{1.61}\\]\n\n\n\n\n\n\nviewof aw = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`a:` })\nviewof bw = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`b:` })\nviewof xmweibull = Inputs.range([1, 100], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nfunction weibull(t,a,b){\n  return a*b*t**(b-1)*Math.exp(-a*t**b);\n}\n\nymaxweibull = d3.max(d3.range(0.1, xmweibull, 0.01).map(t =&gt; weibull(t,aw,bw)))\n\nPlot.plot({\n  x: { domain: [0.1,xmweibull] },\n  y: { domain: [-ymaxweibull*0.1,  ymaxweibull*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    Plot.line(\n      d3\n        .range(0.1, xmweibull, 0.01)\n        .map(t =&gt; [t,  weibull(t,aw,bw)]),{\n        strokeWidth: 3,\n        stroke: \"steelblue\"\n      }\n    ),\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: 0 })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.6: Graph of \\(f_X(x)= ab x^{b-1}e^{-a x^b}\\)\n\n\n\n\n\n\nimport {legend, swatches} from \"@d3/color-legend\"\n\n\nviewof xmin = Inputs.range([0.1, xmax*0.9], { value: 0.1, step: 0.1, label: tex`x_\\mathrm{min}` })\n\nviewof xmax = Inputs.range([1,1000], { value: 10, step: 1, label: tex`x_\\mathrm{max}` })\n\nviewof lexp = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\lambda_{Exp}:` })\n\nviewof agamma = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\alpha_{Gamma}:` })\nviewof lgamma = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\lambda_{Gamma}:` })\n\nviewof muG = Inputs.range([0.1, 10], { value: 0, step: 0.001, label: tex`\\mu_{\\mathcal{N}}:` })\nviewof sigmaG = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\sigma_{\\mathcal{N}}:` })\n\nviewof logmu = Inputs.range([0.1, 10], { value: 0, step: 0.001, label: tex`\\mu_{\\log\\mathcal{N}}:` })\nviewof logsigma = Inputs.range([0.1, 10], { value: 1, step: 0.001, label: tex`\\sigma_{\\log\\mathcal{N}}:` })\n\nviewof apareto = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`\\alpha_{Pareto}:` })\n\nviewof lpareto = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`\\lambda_{Pareto}:` })\n\nviewof aweibull = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`a_{Weibull}:` })\n\nviewof bweibull = Inputs.range([0.1, 10], { value: 0.1, step: 0.001, label: tex`b_{Weibull}:` })\nviewof isExp = Inputs.toggle({label: \"exponential\",value: true})\n\nviewof isGamma = Inputs.toggle({label: \"gamma\",value: true})\n\nviewof isGauss = Inputs.toggle({label: \"normal\",value: true})\n\nviewof isLogGauss = Inputs.toggle({label: \"lognormal\",value: true})\n\nviewof isPareto = Inputs.toggle({label: \"Pareto\",value: true})\n\nviewof isWeibull = Inputs.toggle({label: \"Weibull\",value: true})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyexp = d3.max(d3.range(xmin, xmax, 0.01).map(t =&gt; lexp*Math.exp(-lexp * t)))\nygamma = d3.max(d3.range(xmin, xmax, 0.001).map(t =&gt; gamma(t,agamma,lgamma)))\nygauss = d3.max(d3.range(xmin, xmax, 0.01).map(t =&gt; gauss(t,mu,sigma)))\nyloggauss = d3.max(d3.range(xmin, xmax, 0.001).map(t =&gt; loggauss(t,logmu,logsigma)))\nypareto = d3.max(d3.range(xmin, xmax, 0.01).map(t =&gt; pareto(t,apareto,lpareto)))\nyweibull = d3.max(d3.range(xmin, xmax, 0.01).map(t =&gt; weibull(t,aweibull,bweibull)))\n\ntfull = d3.max([isExp?yexp:0, isGamma?ygamma:0, isGauss?ygauss:0, isPareto?ypareto:0, isWeibull?yweibull:0, isLogGauss?yloggauss:0])\n\nPlot.plot({\n  x: { domain: [xmin, xmax] },\n  y: { domain: [-tfull*0.1,  tfull*1.2] },\n  width: 640,\n  height: 240,\n  marks: [\n    (isExp == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t =&gt; [t, lexp*Math.exp(-lexp*t)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[0]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t =&gt; [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isGamma == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t =&gt; [t, gamma(t,agamma,lgamma)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[1]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t =&gt; [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isGauss == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t =&gt; [t, gauss(t,muG,sigmaG)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[2]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t =&gt; [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isLogGauss == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t =&gt; [t, loggauss(t,logmu,logsigma)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[3]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t =&gt; [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isPareto == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t =&gt; [t, pareto(t,apareto,lpareto)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[4]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t =&gt; [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n    (isWeibull == true)?Plot.line(\n      d3\n        .range(xmin, xmax, 0.01)\n        .map(t =&gt; [t,  weibull(t,aweibull,bweibull)]),{\n        strokeWidth: 3,\n        stroke: d3.schemeCategory10[5]\n      }\n    ):Plot.line(\n      d3\n        .range(0, 0.01, 0.01)\n        .map(t =&gt; [t, t]),{\n        strokeWidth: 0,\n        stroke: d3.schemeCategory10[0]\n      }\n    ),\n      Plot.ruleX([xmin]),\n      Plot.ruleY([0]),\n      Plot.axisX({ y: 0 }),\n      Plot.axisY({ x: xmin })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nswatches({\n  color: d3.scaleOrdinal([tex`\\displaystyle f_X(x)=\\lambda e^{-\\lambda x}`, tex`\\displaystyle f_X(x)=\\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1} e^{-\\lambda x}`, tex`\\displaystyle f_X(x)=\\frac1{\\sigma\\sqrt{2\\pi}}e^{-\\frac1{2\\sigma^2}(x-\\mu)^2}`, tex`\\displaystyle f_X(x)= \\frac{1}{x\\sigma\\sqrt{2\\pi}}\\exp\\biggl(- \\frac{(\\ln x -\\mu)^2}{2\\sigma^2}\\biggr)`, tex`\\displaystyle f_X(x)=\\frac{\\alpha\\lambda^\\alpha}{(\\lambda+x)^{\\alpha+1}}`, tex`\\displaystyle f_X(x)=ab x^{b-1}e^{-a x^b}`], d3.schemeCategory10)\n    })\n\n\n\n\n\n\n\n\nFigure 1.7: Comparison of right tails for the density functions\n\n\n\n\n\n\n\n\n\nNote1.12 Likelihood\n\n\n\nLet \\(X:\\Omega\\to{\\mathbb{R}}\\) be a random variable whose distribution depends on a parameter \\(\\theta\\in{\\mathbb{R}}\\).\nSuppose that we observe the data \\(x_1,\\ldots,x_n\\) which is the output of this random variable \\(X\\) in course of \\(n\\) independent trials.\nIn other words, we can say that we observe that i.i.d.r.v. \\(X_1,\\ldots, X_n\\) with \\(X_i\\sim X\\), \\(1\\leq i\\leq n\\), take certain values: \\(X_1=x_1,\\ldots, X_n=x_n\\).\nThe likelihood, or likelihood function, is the function \\(\\mathcal{L}(\\theta)=\\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n)\\) of the unknown parameter \\(\\theta\\) (given the observed data \\(x_1,\\ldots,x_n\\)) which is equal to:\n\n(if \\(X\\) is a discrete random variable) the probability to observe this data (given the value of the parameter \\(\\theta\\)): \\[\n\\begin{aligned}\n\\mathcal{L}(\\theta)&=\\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n)\\\\\n:&=\\mathbb{P}(X_1=x_1,\\ldots,X_n=x_n\\mid \\theta)\n\\\\&= \\mathbb{P}(X_1=x_1\\mid \\theta)\\ldots \\mathbb{P}(X_n=x_n\\mid \\theta).\n\\end{aligned}\n\\tag{1.62}\\]\n(if \\(X\\) is a continuous random variable with the PDF \\(f_X(x)=f_X(x\\mid\\theta)\\)) \\[\n\\begin{aligned}\n\\mathcal{L}(\\theta)&=\\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n)\\\\\n:&=f_X(x_1\\mid\\theta)f_X(x_2\\mid\\theta)\\ldots f_X(x_n\\mid\\theta).\n\\end{aligned}\n\\tag{1.63}\\]\n\n\n\n\n\n\n\n\n\nNote1.13 Maximum likelihood estimator\n\n\n\nThe maximum likelihood estimator\\(\\theta_*\\) of the parameter \\(\\theta\\) is the argument of the maximum of the likelihood function: \\[\n\\theta_*=\\mathop{\\mathrm{argmax}}_\\theta\\mathcal{L}(\\theta),\n\\tag{1.64}\\] that means that \\[\n\\mathcal{L}(\\theta_*) = \\max_{\\theta}\\mathcal{L}(\\theta).\n\\tag{1.65}\\]\nThe standard approach to find \\(\\theta_*\\) is to consider the log-likelihood function \\[\nL(\\theta):=L(\\theta\\mid x_1,\\ldots,x_n)=\\ln \\mathcal{L}(\\theta\\mid x_1,\\ldots,x_n).\n\\tag{1.66}\\]\nThus, in the discrete case, \\[\nL(\\theta) = \\ln \\mathbb{P}(X_1=x_1\\mid \\theta)+ \\ldots + \\ln \\mathbb{P}(X_n=x_n\\mid \\theta),\n\\tag{1.67}\\] whereas, in the continuous case \\[\nL(\\theta) = \\ln f_X(x_1\\mid\\theta)+ \\ldots + \\ln f_X(x_n\\mid\\theta),\n\\tag{1.68}\\]\nThen \\(\\theta_*\\) is the point of maximum for both \\(\\mathcal{L}\\) and \\(L\\): \\[\n\\theta_*=\\mathop{\\mathrm{argmax}}_\\theta L(\\theta)=\\mathop{\\mathrm{argmax}}_\\theta\\mathcal{L}(\\theta).\n\\tag{1.69}\\]\n\n\n\n\n\n\n\n\nWarning1.14 Example: Discrete case\n\n\n\nLet \\(X\\sim Po(\\lambda)\\), i.e. \\[\n\\mathbb{P}(X=k)=\\frac{\\lambda^k}{k!}e^{-\\lambda}, \\quad k\\geq0.\n\\] Suppose we have a sample of \\(n\\) values of \\(X\\): \\(k_1,\\ldots,k_n\\), and we want to estimate \\(\\lambda\\) (here \\(\\theta=\\lambda\\)). Then \\[\n\\begin{aligned}\n\\mathcal{L}(\\lambda)&=\\mathbb{P}(X=k_1\\mid\\lambda)\\ldots \\mathbb{P}(X=k_n\\mid\\lambda)\\\\\n& = \\frac{\\lambda^{k_1}}{k_1!}e^{-\\lambda}\\cdot\\ldots\\cdot \\frac{\\lambda^{k_n}}{k_n!}e^{-\\lambda}\\\\\n& = \\underbrace{\\frac{1}{k_1!\\ldots k_n!}}_{=: c&gt;0}\\lambda^{k_1+\\ldots+k_n}e^{-\\lambda n},\n\\end{aligned}\n\\] and therefore, \\[\n\\begin{aligned}\nL(\\lambda)&=\\ln\\mathcal{L}(\\lambda)\n\\\\& = \\ln c + (k_1+\\ldots+k_n)\\ln\\lambda-\\lambda n.\n\\end{aligned}\n\\] Then \\[\nL'(\\lambda) = \\frac{k_1+\\ldots+k_n}{\\lambda}-n,\n\\] and hence, \\(L'(\\lambda)=0\\) iff \\[\n{\\lambda = \\frac{k_1+\\ldots+k_n}{n}}.\n\\]\nSince \\[\nL''(\\lambda) = (L'(\\lambda))'=-\\frac{k_1+\\ldots+k_n}{\\lambda^2}&lt;0,\n\\] the found value \\(\\lambda_* = \\frac{k_1+\\ldots+k_n}{n}\\) is the point of maximum of \\(L\\), hence, it is the maximum likelihood estimator for the parameter \\(\\lambda\\).\n\n\n\n\n\n\n\n\nTip1.15 Remark: Relation to the LLN\n\n\n\nNote that, by the law of large numbers (LLN), if \\(X_i\\sim X\\sim Po(\\lambda)\\), \\(i\\in\\mathbb{N}\\), then \\[\n\\frac{X_1+\\ldots+X_n}{n} \\to \\mathbb{E}(X)=\\lambda, \\qquad n\\to\\infty.\n\\] In other words, the maximum likelihood estimator converges to the theoretical value is the size of the sample converges to infinity.\n\n\n\n\n\n\n\n\nWarning1.16 Example: Continuous case\n\n\n\nLet \\(X\\sim Exp(\\lambda)\\). Suppose we have a sample of \\(n\\) values of \\(X\\): \\(x_1,\\ldots,x_n\\). Then \\[\n\\begin{aligned}\n\\mathcal{L}(\\lambda) &= f_X(x_1\\mid \\lambda)\\ldots f_X(x_n\\mid \\lambda)\\\\\n&=\\lambda e^{-\\lambda x_1}\\ldots \\lambda e^{-\\lambda x_n}= \\lambda^n e^{-\\lambda(x_1+\\ldots +x_n)}.\n\\end{aligned}\n\\]\nTherefore, \\[\n\\begin{aligned}\nL(\\lambda)&= \\ln \\mathcal L(\\lambda)\n\\\\&= n \\ln \\lambda - \\lambda(x_1+\\ldots +x_n),\n\\end{aligned}\n\\] so \\(L'(\\lambda)=0\\) iff \\[\n\\begin{gathered}\n\\frac{n}{\\lambda} -(x_1+\\ldots +x_n)=0,\\\\\n{\\lambda_*= \\frac{n}{x_1+\\ldots +x_n}}.\n\\end{gathered}\n\\] Note that \\(L''(\\lambda)=-\\dfrac{n}{\\lambda^2}\\), in particular, \\(L''(\\lambda_*)&lt;0\\), i.e. \\(\\lambda_*\\) is indeed the maximum likelihood estimator for the parameter \\(\\lambda\\).\nFinally, note that if \\(X_i\\sim X\\sim Exp(\\lambda)\\), then, by LLN, \\[\n\\frac{X_1+\\ldots+X_n}{n}\\to \\mathbb{E}(X)=\\frac{1}{\\lambda}, \\qquad n\\to\\infty.\n\\]\n\n\n\n\n\n\n\n\nNote1.17 The method of moments\n\n\n\nIf the number of parameters is bigger than \\(1\\), the maximum likelihood estimation may be challenging as one would need to find the point of maximum for a function of several variables (though it can be still effectively done numerically).\nAnother method to estimate unknown parameters of the distribution is the method of moments. Namely, assuming that we have a sample of \\(n\\) values \\(x_1,\\ldots,x_n\\) of a random variable \\(X\\) which is believed to be followed a probability distribution which depends on \\(k\\) unknown parameters \\(\\theta=(\\theta_1,\\ldots,\\theta_k)\\).\nThen, we consider the first \\(k\\) moments of the random variable \\(X\\) (i.e. the moments of the population): \\[\nm_j = \\mathbb{E}(X^j\\mid \\theta), \\qquad 1\\leq j\\leq k.\n\\]\nNext, for the given data \\(x_1,\\ldots,x_n\\), we consider \\(k\\) averaged moments (i.e. the moments of the sample): \\[\n\\overline{m}_j = \\frac{1}{n}\\sum_{l=1}^n x_l ^j, \\qquad 1\\leq j\\leq k.\n\\]\nAfter this, we need to equate \\(k\\) quantities: \\[\nm_j = \\overline{m}_j, \\qquad 1\\leq j\\leq k,\n\\] to determine \\(k\\) unknown parameters \\(\\theta_1,\\ldots,\\theta_k\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Loss distributions</span>"
    ]
  }
]